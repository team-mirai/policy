# AI活用の大原則

## AI活用の基本原則
AIの持つ脆弱性や透明性の課題に対応するため、以下の原則をすべてのAI関連政策の根幹に据えます。

1.  **責任の所在の明確化:** 政策に利用するAIを開発・提供している組織がどこなのかを常に明示し、システムが何らかの損害を与えた場合の責任の所在を、あらかじめ明確に定めておくこと。
2.  **エラー原因の説明責任:** システムに技術的なエラーや想定外の挙動が発生した場合、速やかにその原因を調査・公表し、国民に対して説明責任を果たすこと。
3.  **堅牢なシステム設計と俯瞰性の確保:** AIの基本的な利用目的やガードレールを国民がいつでも確認できる（俯瞰できる）状態を保つこと。そして、特定のプロンプトや会話によって、システムの公平性や安定性といった根幹が揺らぐことのない、堅牢なシステム設計を義務付けること。

## STEP 0-X: 破滅級（Existential）リスク管理条項

1.  **フロンティアモデルのライセンス制**  
    年間演算量（FLOPs） ≧ 10²⁶ を要する AI モデルは、核施設並みの事前国家ライセンス取得と稼働監査を義務付けます。
2.  **Catastrophic Red-Team 演習の定期実施**  
    防衛省・外務省・内閣サイバーセキュリティセンターを統合した机上演習を年 1 回以上実施し、偽情報起因の軍事誤作動や自動サイバー反撃連鎖シナリオを検証します。
3.  **デュアルキーと物理キルスイッチの義務化**  
    重要インフラ・軍事用途 AI には、人間 2 名の同時承認（デュアルキー）と、オフライン遮断回路（物理的 kill switch）を法定化します。
4.  **重大事故時のフルログ強制開示（24 時間ルール）**  
    異常挙動が深度 5 以上（破滅級リスク閾値）と判定された場合、24 時間以内に暫定レポートと全量ログを AI 監督庁へ自動提出し、同時に公報で告知します。
5.  **AI 安全性基金（資金スキームは後日協議）**  
    財源構成は党内外の専門家協議により確定するものとし、ここでは詳細を定めません。
